{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This notebook lays out one possible method for generating word embeddings - that is, a way of representing of a dictionary of words (drawn from a training set) as numerical data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec and t-SNE\n",
    "\n",
    "\n",
    "[_Word embeddings_](http://arxiv.org/pdf/1301.3781.pdf) provide a meaningful representation of text. Word embeddings, called such because they involve embedding a word in some high-dimensional space, that is, they map a word to some vector. Word embeddings are learned for a particular task, so they end up being meaningful representations.\n",
    "\n",
    "For example, the relationships between words can be meaningful (image from the [TensorFlow documentation]((https://www.tensorflow.org/versions/r0.9/tutorials/word2vec/index.html)):\n",
    "\n",
    "![Word embedding relationships](https://www.tensorflow.org/versions/r0.9/images/linear-relationships.png)\n",
    "\n",
    "A notable property that emerges is that vector arithmetic can also be meaningful. Perhaps the most well-known example of this is:\n",
    "\n",
    "$$\n",
    "\\text{king} - \\text{man} + \\text{woman} = \\text{queen}\n",
    "$$\n",
    "\n",
    "So the positioning of these words in this space actually tells us something about *how these words are used*.\n",
    "\n",
    "This allows us to do things like find the most similar words by looking at the closest words. You can project the resulting embeddings down to 2D so that use patterns can be visualized. \n",
    "\n",
    "- try using t-SNE (\"t-Distributed Stochastic Neighbor Embedding\") for this, which is a dimensionality reduction method that visualizes high-dimension data. \n",
    "\n",
    "As mentioned earlier, these word embeddings are trained to help with a particular task, which is learned through a neural network. Two tasks developed for training embeddings are _CBOW_ (\"Continuous Bag Of Words\") and _skip-grams_; together these methods of learning word embeddings are called \"Word2Vec\".\n",
    "\n",
    "For the CBOW task, we take the context words (the words around the target word) and give the target word. We want to predict whether or not the target word belongs to the context.\n",
    "\n",
    "A skip-gram is basically the inverse: we take the target word (the \"pivot\"), then give the context. We want to predict whether or not the context belongs to the word.\n",
    "\n",
    "They are quite similar but have different properties, e.g. CBOW works better on smaller datasets, where as skip-grams works better for larger ones. In any case, the idea with word embeddings is that they can be trained to help with any task.\n",
    "\n",
    "We're going to be using the skip-gram task here.\n",
    "\n",
    "## Corpus\n",
    "\n",
    "We need a reasonably-sized text corpus to learn from. Here we'll use State of the Union addresses retrieved from [The American Presidency Project](http://www.presidency.ucsb.edu/sou.php). These addresses tend to use similar patterns so we should be able to learn some decent word embeddings. Since the skip-gram task looks at context, texts that use words in a consistent way (i.e. in consistent contexts) we'll be able to learn better.\n",
    "\n",
    "[The corpus is available here](/guides/data/sotu.tar.gz) as a compressed archive of .txt files. The texts were preprocessed a bit (mainly removing URL-encoded characters). (nb: this isn't the complete collection of texts but enough to work with here).\n",
    "\n",
    "## Skip-grams\n",
    "\n",
    "Before we go any further, let's get a bit more concrete about what the skip-gram task is.\n",
    "\n",
    "Let's consider the sentence \"I think cats are cool\".\n",
    "\n",
    "The skip-gram task is as follows:\n",
    "\n",
    "- We take a word, e.g. `'cats'`, which we'll represent as $w_i$. We feed this as input into our neural network.\n",
    "- We take the word's context, e.g. `['I', 'think', 'are', 'cool']`. We'll represent this as $\\{w_{i-2}, w_{i-1}, w_{i+1}, w_{i+2}\\}$ and we also feed this into our neural network.\n",
    "- Then we just want our network to predict (i.e. classify) whether or not $\\{w_{i-2}, w_{i-1}, w_{i+1}, w_{i+2}\\}$ is the true context of $w_i$.\n",
    "\n",
    "For this particular example we'd want the network to output 1 (i.e. yes, that is the true context).\n",
    "\n",
    "If we set $w_i$ to 'frogs', then we'd want the network output 0. In our one sentence corpus, `['I', 'think', 'are', 'cool']` is not the true context for 'frogs'. Sorry frogs üê∏.\n",
    "\n",
    "## Building the model\n",
    "\n",
    "We'll use `keras` to build the neural network that we'll use to learn the embeddings.\n",
    "\n",
    "First we'll import everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Flatten, Activation, Merge\n",
    "from keras.preprocessing.text import Tokenizer, base_filter\n",
    "from keras.preprocessing.sequence import skipgrams, make_sampling_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "depending on which environment you're running this from, you may find yourself needing to upgrade one of these libraries, which you can do by opening an ipython terminal, launching python, and typing `pip install [name of library] --upgrade`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then load in our data. We're actually going to define a generator to load our data in on-demand; this way we'll avoid having all our data sitting around in memory when we don't need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "text_files = glob('sotu/*.txt')\n",
    "\n",
    "def text_generator():\n",
    "    for path in text_files:\n",
    "        with open(path, 'r') as f:\n",
    "            yield f.read()\n",
    "            \n",
    "len(text_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go any further, we need to map the words in our corpus to numbers, so that we have a consistent way of referring to them. First we'll fit a tokenizer to the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# our corpus is small enough where we\n",
    "# don't need to worry about this, but good practice\n",
    "max_vocab_size = 50000\n",
    "\n",
    "# `filters` specify what characters to get rid of\n",
    "# `base_filter()` includes basic punctuation;\n",
    "# I like to extend it with common unicode punctuation\n",
    "tokenizer = Tokenizer(nb_words=max_vocab_size,\n",
    "                      filters=base_filter()+'‚Äú‚Äù‚Äì')\n",
    "\n",
    "# fit the tokenizer\n",
    "tokenizer.fit_on_texts(text_generator())\n",
    "\n",
    "# we also want to keep track of the actual vocab size\n",
    "# we'll need this later\n",
    "# note: we add one because `0` is a reserved index in keras' tokenizer\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the tokenizer knows what tokens (words) are in our corpus and has mapped them to numbers. The `keras` tokenizer also indexes them in order of frequency (most common first, i.e. index 1 is usually a word like \"the\"), which will come in handy later.\n",
    "\n",
    "At this point, let's define the dimensions of our embeddings. It's up to you and your task to choose this number. Like many neural network hyperparameters, you may just need to play around with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the model. When I described the skip-gram task, I mentioned two inputs: the target word (also called the \"pivot\") and the context. So we're going to build two separate models for each input and then merge them into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pivot_model = Sequential()\n",
    "pivot_model.add(Embedding(vocab_size, embedding_dim, input_length=1))\n",
    "\n",
    "context_model = Sequential()\n",
    "context_model.add(Embedding(vocab_size, embedding_dim, input_length=1))\n",
    "\n",
    "# merge the pivot and context models\n",
    "model = Sequential()\n",
    "model.add(Merge([pivot_model, context_model], mode='dot', dot_axes=2))\n",
    "model.add(Flatten())\n",
    "\n",
    "# the task as we've framed it here is\n",
    "# just binary classification,\n",
    "# so we want the output to be in [0,1],\n",
    "# and we can use binary crossentropy as our loss\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "\n",
    "# used to sample words (indices)\n",
    "sampling_table = make_sampling_table(vocab_size)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    loss = 0\n",
    "    for seq in tokenizer.texts_to_sequences_generator(text_generator()):\n",
    "        # generate skip-gram training examples\n",
    "        # - `couples` consists of the pivots (i.e. target words) and surrounding contexts\n",
    "        # - `labels` represent if the context is true or not\n",
    "        # - `window_size` determines how far to look between words\n",
    "        # - `negative_samples` specifies the ratio of negative couples\n",
    "        #    (i.e. couples where the context is false)\n",
    "        #    to generate with respect to the positive couples;\n",
    "        #    i.e. `negative_samples=4` means \"generate 4 times as many negative samples\"\n",
    "        couples, labels = skipgrams(seq, vocab_size, window_size=5, negative_samples=4, sampling_table=sampling_table)\n",
    "        if couples:\n",
    "            pivot, context = zip(*couples)\n",
    "            pivot = np.array(pivot, dtype='int32')\n",
    "            context = np.array(context, dtype='int32')\n",
    "            labels = np.array(labels, dtype='int32')\n",
    "            loss += model.train_on_batch([pivot, context], labels)\n",
    "    print('epoch %d, %0.02f'%(i, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait a few minutes for training...\n",
    "\n",
    "Now we can extract the embeddings, which are just the weights of the pivot embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to set aside the tokenizer's `word_index` for later use (so we can get indices for words) and also create a reverse word index (so we can get words from indices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "reverse_word_index = {v: k for k, v in word_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for learning the embeddings. Now we can try using them.\n",
    "\n",
    "## Getting similar words\n",
    "\n",
    "Each word embedding is just a mapping of a word to some point in space. So if we want to find words similar to some target word, we literally just need to look at the closest embeddings to that target word's embedding.\n",
    "\n",
    "An example will make this clearer.\n",
    "\n",
    "First, let's write a simple function to retrieve an embedding for a word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding(word):\n",
    "    idx = word_index[word]\n",
    "    # make it 2d\n",
    "    return embeddings[idx][:,np.newaxis].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can define a function to get a most similar word for an input word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "ignore_n_most_common = 50\n",
    "\n",
    "def get_closest(word):\n",
    "    embedding = get_embedding(word)\n",
    "\n",
    "    # get the distance from the embedding\n",
    "    # to every other embedding\n",
    "    distances = cdist(embedding, embeddings)[0]\n",
    "\n",
    "    # pair each embedding index and its distance\n",
    "    distances = list(enumerate(distances))\n",
    "\n",
    "    # sort from closest to furthest\n",
    "    distances = sorted(distances, key=lambda d: d[1])\n",
    "\n",
    "    # skip the first one; it's the target word\n",
    "    for idx, dist in distances[1:]:\n",
    "        # ignore the n most common words;\n",
    "        # they can get in the way.\n",
    "        # because the tokenizer organized indices\n",
    "        # from most common to least, we can just do this\n",
    "        if idx > ignore_n_most_common:\n",
    "            return reverse_word_index[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's give it a try (you may get different results):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advancement\n",
      "appeasement\n",
      "very\n",
      "undo\n",
      "intentioned\n"
     ]
    }
   ],
   "source": [
    "print(get_closest('freedom'))\n",
    "print(get_closest('justice'))\n",
    "print(get_closest('america'))\n",
    "print(get_closest('citizens'))\n",
    "print(get_closest('citizen'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do words have relations?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
