{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "This notebook lays out one possible method for generating **word embeddings**.  \n",
    "\n",
    "[_Word embeddings_](http://arxiv.org/pdf/1301.3781.pdf) provide meaningful representations of words as they are used in context. Word embeddings [embed](https://en.wikipedia.org/wiki/Embedding) words in high-dimensional space, mapping each word or phrase to its own [vector](https://en.wikipedia.org/wiki/Vector_space). Word embeddings are trained to help with a specific task, which determines its meaningulness as a representation.\n",
    "\n",
    "For example, the relationships between vectors can be meaningful (image from the [TensorFlow documentation]((https://www.tensorflow.org/versions/r0.9/tutorials/word2vec/index.html)):\n",
    "\n",
    "![Word embedding relationships](https://www.tensorflow.org/versions/r0.9/images/linear-relationships.png)\n",
    "\n",
    "Arithmetic with vectors can also be meaningful. Perhaps the most well-known example of this is:\n",
    "\n",
    "$$\n",
    "\\text{king} - \\text{man} + \\text{woman} = \\text{queen}\n",
    "$$\n",
    "\n",
    "The positioning of these vectors in this space actually tells us something about *how these words are used*.\n",
    "\n",
    "This allows us to do things like find the most similar words by looking at the closest words. \n",
    "\n",
    "As mentioned earlier, these word embeddings are trained to help with a particular task, which is learned through a neural network. Two tasks developed for training embeddings are _CBOW_ (\"Continuous Bag Of Words\") and _skip-grams_; together these methods of learning word embeddings are called \"[Word2Vec](https://en.wikipedia.org/wiki/Word2vec)\".\n",
    "\n",
    "For the _CBOW_ task, we take the context words (the words around the target word) and give the target word. We want to predict whether or not the target word belongs to the context.\n",
    "\n",
    "A _skip-gram_ is basically the inverse: we take the target word (also called the \"pivot\"), then give the context. We want to predict whether or not the context belongs to the word.\n",
    "\n",
    "They are quite similar but have different properties, e.g. _CBOW_ works better on small datasets, while _skip-grams_ work better for larger ones. In any case, the idea with word embeddings is that they can be trained to help with any task.\n",
    "\n",
    "For this example, we're going to be working on the _skip-gram_ task.\n",
    "\n",
    "## Corpus\n",
    "\n",
    "We need a reasonably-sized text corpus to learn from. Here we'll use State of the Union addresses retrieved from [The American Presidency Project](http://www.presidency.ucsb.edu/sou.php). These addresses tend to use similar patterns so we should be able to learn some decent word embeddings. Since the skip-gram task looks at context, texts that use words in a consistent way (i.e. in consistent contexts) are easier to learn from.\n",
    "\n",
    "[The corpus is available here](https://github.com/publicityreform/findbyimage/raw/master/notebooks/word2vec/sotu.zip) as a compressed archive of .txt files. Download and un-zip this file and place it in the same directory as this notebook. The texts were preprocessed a bit (mainly removing URL-encoded characters). (nb: this isn't the complete collection of texts but enough to work with here).\n",
    "\n",
    "## Skip-grams\n",
    "\n",
    "Before we go any further, let's get a bit more concrete about what the skip-gram task is.\n",
    "\n",
    "Let's consider the sentence `\"I think cats are cool\"`.\n",
    "\n",
    "The skip-gram task is as follows:\n",
    "\n",
    "- We take a word, e.g. `'cats'`, which we'll represent as $w_i$. We feed this as input into our neural network.\n",
    "- We take the word's context, e.g. `['I', 'think', 'are', 'cool']`. We'll represent this as $\\{w_{i-2}, w_{i-1}, w_{i+1}, w_{i+2}\\}$ and we also feed this into our neural network.\n",
    "- Then we just want our network to predict (i.e. classify) whether or not $\\{w_{i-2}, w_{i-1}, w_{i+1}, w_{i+2}\\}$ is the true context of $w_i$.\n",
    "\n",
    "For this particular example we'd want the network to output 1 (i.e. yes, that is the true context).\n",
    "\n",
    "If we set $w_i$ to 'frogs', then we'd want the network output 0. In our one sentence corpus, `['I', 'think', 'are', 'cool']` is not the true context for 'frogs'. Sorry frogs üê∏.\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Import dependencies:\n",
    "\n",
    "For this example, we'll use [keras](https://keras.io/) to build the neural network that we'll use to learn the embeddings. keras is a high-level library that can use either a [tensorflow](https://www.tensorflow.org) or [theano](http://www.deeplearning.net/software/theano/) backend to handle low-level tasks. to switch between tensorflow and theano backends, edit the keras configuration file at `$HOME/.keras/keras.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Flatten, Activation, Merge\n",
    "from keras.preprocessing.text import Tokenizer, base_filter\n",
    "from keras.preprocessing.sequence import skipgrams, make_sampling_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...Depending on which environment you're running this from, you may find yourself needing to upgrade one of these libraries, which you can do by opening an ipython terminal, launching python, and typing `pip install [name of library] --upgrade`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Load data for training. \n",
    "\n",
    "change `'sotu/'` to the name and path of the folder containing the txt files that you want to train the model on. \n",
    "\n",
    "if the folder is in a directory other than the one this notebook is saved in, add an absolute path (e.g. `'/sharedfolder/datasets/folder/*txt'`)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sucessfully located 84 text files to use as training data!\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "text_files = glob('sotu/*.txt') \n",
    "# locates all .txt files in the folder 'sotu' \n",
    "# (in the same directory as this notebook)\n",
    "\n",
    "# define a `text_generator` here, \n",
    "# so that data can be loaded on-demand, \n",
    "# avoiding having all data in memory unless it's needed:\n",
    "def text_generator():\n",
    "    for path in text_files:\n",
    "        with open(path, 'r') as f:\n",
    "            yield f.read()\n",
    "   \n",
    "files = (len(text_files))\n",
    "if files == 0:\n",
    "    print\"something's not right - check the file path?\"\n",
    "else:\n",
    "    print\"sucessfully located\",len(text_files),\"text files to use as training data!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go any further, we need to map each word in our corpus to a number, so that we have a consistent way of referring to them. To do this, we'll fit a `tokenizer` to the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 15288 unique words\n"
     ]
    }
   ],
   "source": [
    "max_vocab_size = 50000\n",
    "# setting an upper limit just in case\n",
    "\n",
    "# `filters` specify what characters to get rid of\n",
    "# `base_filter()` includes basic punctuation \n",
    "tokenizer = Tokenizer(nb_words=max_vocab_size, filters=base_filter()+'')\n",
    "\n",
    "# fit the tokenizer\n",
    "tokenizer.fit_on_texts(text_generator())\n",
    "\n",
    "# we also want to keep track of the actual vocab size:\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# note: we add one because `0` is a reserved index in keras' tokenizer\n",
    "\n",
    "print \"found\",vocab_size,\"unique words\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the tokenizer knows what tokens (words) are in our corpus and has mapped them to numbers. The `keras` tokenizer also indexes them in order of frequency (most common first, i.e. index 1 is usually a word like \"the\"), which will come in handy later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define the model. \n",
    "\n",
    "First, let's set the hyperparameters (higher-level settings that determine how the model trains): \n",
    "\n",
    "- the dimensions of each embedding \n",
    "- the number of training epochs\n",
    "- the [loss function](https://keras.io/losses/) used \n",
    "- the [activation function](https://keras.io/activations/) used\n",
    "- the [optimizer type](https://keras.io/optimizers/)\n",
    "\n",
    "You may just need to play around to get results that fit well to the task at hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "n_epochs = 60\n",
    "# Higher numbers will add time to the training, \n",
    "# lower numbers may give wildly inaccurate (er, abstract?) results.\n",
    "\n",
    "loss_function = 'binary_crossentropy'\n",
    "activation_function = 'sigmoid'\n",
    "optimizer_type = 'adam'\n",
    "# the task as we are framing it is to answer the question: \n",
    "# \"do the context words match the target word or not?\"\n",
    "# because this is a binary classification, \n",
    "# we want the output to be normalized to [0,1] (sigmoid will work),\n",
    "# and we can use 'binary crossentropy' as our loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the \"skip-gram\" tast, build two separate models (one for the target word (also called the \"pivot\"), and one for the context words), and then merge them into one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pivot_model = Sequential()\n",
    "pivot_model.add(Embedding(vocab_size, embedding_dim, input_length=1))\n",
    "\n",
    "context_model = Sequential()\n",
    "context_model.add(Embedding(vocab_size, embedding_dim, input_length=1))\n",
    "\n",
    "# merge the pivot and context models\n",
    "model = Sequential()\n",
    "model.add(Merge([pivot_model, context_model], mode='dot', dot_axes=2))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Activation(activation_function))\n",
    "model.compile(optimizer=optimizer_type, loss=loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Train the model.\n",
    "\n",
    "run the code below to load pre-trained weights. you can download weights learned on this model over 60 epochs [here](https://drive.google.com/open?id=0B9WYEmYh8AIXTXRTUU5FUlFQdmM) \n",
    "\n",
    "\n",
    "**OR** skip to the following step to train the model from scratch (takes a few minutes): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded pre-trained weights\n"
     ]
    }
   ],
   "source": [
    "# skip this if you want to train the model from scratch!\n",
    "\n",
    "# run this to load pre-trained weights!\n",
    "# change the filepath 'weights-60epochs.hdf5' to another path if necessary\n",
    "\n",
    "model.load_weights('weights-60epochs.hdf5')\n",
    "print\"successfully loaded pre-trained weights\"\n",
    "\n",
    "# then skip to \"Step 5: Extract the Embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, 17.30\n",
      "epoch 1, 17.17\n",
      "epoch 2, 17.03\n",
      "epoch 3, 16.90\n",
      "epoch 4, 16.72\n",
      "epoch 5, 16.54\n",
      "epoch 6, 16.32\n",
      "epoch 7, 16.13\n",
      "epoch 8, 15.90\n",
      "epoch 9, 15.65\n",
      "epoch 10, 15.43\n",
      "epoch 11, 15.16\n",
      "epoch 12, 14.94\n",
      "epoch 13, 14.62\n",
      "epoch 14, 14.33\n",
      "epoch 15, 14.10\n",
      "epoch 16, 13.80\n",
      "epoch 17, 13.55\n",
      "epoch 18, 13.26\n",
      "epoch 19, 13.02\n",
      "epoch 20, 12.73\n",
      "epoch 21, 12.48\n",
      "epoch 22, 12.27\n",
      "epoch 23, 12.01\n",
      "epoch 24, 11.78\n",
      "epoch 25, 11.59\n",
      "epoch 26, 11.33\n",
      "epoch 27, 11.15\n",
      "epoch 28, 11.01\n",
      "epoch 29, 10.81\n",
      "epoch 30, 10.66\n",
      "epoch 31, 10.45\n",
      "epoch 32, 10.29\n",
      "epoch 33, 10.16\n",
      "epoch 34, 9.99\n",
      "epoch 35, 9.89\n",
      "epoch 36, 9.74\n",
      "epoch 37, 9.64\n",
      "epoch 38, 9.49\n",
      "epoch 39, 9.33\n",
      "epoch 40, 9.26\n",
      "epoch 41, 9.10\n",
      "epoch 42, 9.07\n",
      "epoch 43, 8.95\n",
      "epoch 44, 8.84\n",
      "epoch 45, 8.77\n",
      "epoch 46, 8.66\n",
      "epoch 47, 8.61\n",
      "epoch 48, 8.49\n",
      "epoch 49, 8.43\n",
      "epoch 50, 8.39\n",
      "epoch 51, 8.29\n",
      "epoch 52, 8.18\n",
      "epoch 53, 8.18\n",
      "epoch 54, 8.06\n",
      "epoch 55, 8.01\n",
      "epoch 56, 7.94\n",
      "epoch 57, 7.94\n",
      "epoch 58, 7.88\n",
      "epoch 59, 7.77\n"
     ]
    }
   ],
   "source": [
    "# skip this if you want to use pre-trained weights!\n",
    "\n",
    "# run this to train the model from scratch!\n",
    "\n",
    "# used to sample words (indices)\n",
    "sampling_table = make_sampling_table(vocab_size)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    loss = 0\n",
    "    for seq in tokenizer.texts_to_sequences_generator(text_generator()):\n",
    "        # generate skip-gram training examples\n",
    "        # - `couples` consists of the pivots (i.e. target words) and surrounding contexts\n",
    "        # - `labels` represent if the context is true or not\n",
    "        # - `window_size` determines how far to look between words\n",
    "        # - `negative_samples` specifies the ratio of negative couples\n",
    "        #    (i.e. couples where the context is false)\n",
    "        #    to generate with respect to the positive couples;\n",
    "        #    i.e. `negative_samples=4` means \"generate 4 times as many negative samples\"\n",
    "        couples, labels = skipgrams(seq, vocab_size, window_size=5, negative_samples=4, sampling_table=sampling_table)\n",
    "        if couples:\n",
    "            pivot, context = zip(*couples)\n",
    "            pivot = np.array(pivot, dtype='int32')\n",
    "            context = np.array(context, dtype='int32')\n",
    "            labels = np.array(labels, dtype='int32')\n",
    "            loss += model.train_on_batch([pivot, context], labels)\n",
    "    print('epoch %d, %0.02f'%(i, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait a few minutes for training...\n",
    "\n",
    "then save the trained weights to re-use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully saved trained weights\n"
     ]
    }
   ],
   "source": [
    "model.save_weights('weights2.hdf5')\n",
    "print\"successfully saved trained weights\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Extract the embeddings\n",
    "    \n",
    "I.e. the weights of the pivot embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to set aside the tokenizer's `word_index` and `reverse_word_index` (so we can look up indices for words and words from indices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "reverse_word_index = {v: k for k, v in word_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for learning the embeddings. Now we can try using them:\n",
    "\n",
    "## Getting similar words\n",
    "\n",
    "Each *word embedding* is a mapping of a *specific word* to a *point in space*. If we want to find words that are similar (in terms of how they are used) to some target word, we look for embeddings that are nearby to the point in space where the target word's embedding is mapped.\n",
    "\n",
    "An example will make this clearer.\n",
    "\n",
    "First, let's write a simple function to retrieve an embedding for a word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding(word):\n",
    "    idx = word_index[word]\n",
    "    # make it 2d\n",
    "    return embeddings[idx][:,np.newaxis].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can define a function to get a most similar word for an input word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "ignore_n_most_common = 50\n",
    "\n",
    "def get_closest(word):\n",
    "    embedding = get_embedding(word)\n",
    "\n",
    "    # get the distance from the embedding\n",
    "    # to every other embedding\n",
    "    distances = cdist(embedding, embeddings)[0]\n",
    "\n",
    "    # pair each embedding index and its distance\n",
    "    distances = list(enumerate(distances))\n",
    "\n",
    "    # sort from closest to furthest\n",
    "    distances = sorted(distances, key=lambda d: d[1])\n",
    "\n",
    "    # skip the first one; it's the target word\n",
    "    for idx, dist in distances[1:]:\n",
    "        # ignore the n most common words;\n",
    "        # they can get in the way.\n",
    "        # because the tokenizer organized indices\n",
    "        # from most common to least, we can just do this\n",
    "        if idx > ignore_n_most_common:\n",
    "            return reverse_word_index[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's give it a try (you may get different results):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freedom ~ peace\n",
      "justice ~ infused\n",
      "america ~ country\n",
      "history ~ nation\n",
      "citizen ~ every\n"
     ]
    }
   ],
   "source": [
    "print'freedom ~',(get_closest('freedom'))\n",
    "print'justice ~',(get_closest('justice'))\n",
    "print'america ~',(get_closest('america'))\n",
    "print'history ~',(get_closest('history'))\n",
    "print'citizen ~',(get_closest('citizen'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do words have relations?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
