
most RNN’s belong to the general category of “generative” models, in that these models provide a departure from strict classification or prediction tasks. 

allison parrish: “When we teach computers to write, the computers don’t replace us any more than pianos replace pianists—in a certain way, they become our pens, and we become more than writers. We become writers of writers.”

[_”recurrent net dreams up fake chinese characters”_ from otoro blog](http://blog.otoro.net/2015/12/28/recurrent-net-dreams-up-fake-chinese-characters-in-vector-format-with-tensorflow/)

[ross goodwin _”adventures in narrated reality”_](https://medium.com/artists-and-machine-intelligence/adventures-in-narrated-reality-6516ff395ba3)

https://arstechnica.co.uk/the-multiverse/2016/06/sunspring-movie-watch-written-by-ai-details-interview/


[intro to RNN’s from “machine learning is fun”](https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3)

[intro to RNN’s from wildML](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)

[intro to RNN’s from chris olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) (chinese translation [here](http://www.jianshu.com/p/9dc9f41f0b29))

[compared](http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139) to maximum-likelihood language models (e.g. Markov chains), RNN’s manage to balance structure at different scales (context-awareness) while generating completely new material (no copying).

[intro to RNN’s from andrej karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)


[collection of many resources related to learning about RNN’s](https://handong1587.github.io/deep_learning/2015/10/09/rnn-and-lstm.html)

[LSTM’s vs. Bag-Of-Words Model](https://metamind.io/research/learning-when-to-skim-and-when-to-read)

[attention and memory](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)

[augmented RNN’s (more on attention and memory)](http://distill.pub/2016/augmented-rnns/)


[generating click-bait news stories](http://clickotron.com/about)


[generating new choreography from existing dances](http://peltarion.com/creative-ai)
[using RNN’s to ](https://twitter.com/evolvingstuff/status/713149843481317376)
or 
[structured feature learning](https://twitter.com/alexjc/status/716549734371102720/photo/1)

code repositories:
* [word-rnn](https://github.com/larspars/word-rnn)
* [char-rnn](https://github.com/karpathy/char-rnn)
* [sketch-rnn]
* [pixel-rnn](https://github.com/tensorflow/magenta/blob/master/magenta/reviews/pixelrnn.md)
* [neural storyteller](https://github.com/ryankiros/neural-storyteller) (more about this [here](https://medium.com/artists-and-machine-intelligence/a-journey-through-multiple-dimensions-and-transformations-in-space-the-final-frontier-d8435d81ca51#cf7d)
* [lexiconjure - generates definitions for made-up words](https://github.com/rossgoodwin/lexiconjure)
* [torch-rnn server](https://github.com/robinsloan/torch-rnn-server)



temperature
mixture density networks

https://www.theverge.com/a/luka-artificial-intelligence-memorial-roman-mazurenko-bot

https://medium.com/@katierosepipkin/a-long-history-of-generated-poetics-cutups-from-dickinson-to-melitzah-fce498083233
